{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from scipy import stats\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(therapist_resampled, patient_resampled, feature, filename):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(therapist_resampled.index, therapist_resampled[feature], label='Therapist', alpha=0.7)\n",
    "    plt.plot(patient_resampled.index, patient_resampled[feature], label='Patient', alpha=0.7)\n",
    "    plt.title(f'{feature} Time Series for {filename}')\n",
    "    #plt.title(f'{feature} Time Series Example')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel(feature)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'~/WAI/TimeSeriesFigs/{filename}_{feature}_timeseries.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(csv_file):\n",
    "    df = pd.read_csv(csv_file, delimiter=';')\n",
    "    df = df[df['Speaker'] != 'NAN'] #filter out nan speakers\n",
    "    \n",
    "    features = {}\n",
    "\n",
    "    features['filename'] = os.path.basename(csv_file)\n",
    "    \n",
    "    for speaker in df['Speaker'].unique():\n",
    "        speaker_df = df[df['Speaker'] == speaker]\n",
    "        \n",
    "        features[f'{speaker}_avg_arousal'] = speaker_df['Arousal'].mean()\n",
    "        features[f'{speaker}_avg_valence'] = speaker_df['Valence'].mean()\n",
    "        \n",
    "        features[f'{speaker}_var_arousal'] = speaker_df['Arousal'].var()\n",
    "        features[f'{speaker}_var_valence'] = speaker_df['Valence'].var()\n",
    "        \n",
    "        features[f'{speaker}_max_arousal'] = speaker_df['Arousal'].max()\n",
    "        features[f'{speaker}_max_valence'] = speaker_df['Valence'].max()\n",
    "        \n",
    "        features[f'{speaker}_min_arousal'] = speaker_df['Arousal'].min()\n",
    "        features[f'{speaker}_min_valence'] = speaker_df['Valence'].min()\n",
    "        \n",
    "        features[f'{speaker}_total_time'] = (speaker_df['End Time'] - speaker_df['Start Time']).sum()\n",
    "        features[f'{speaker}_num_utterances'] = len(speaker_df)\n",
    "        \n",
    "        sentiment_counts = speaker_df['Sentiment'].value_counts(normalize=True)\n",
    "        features[f'{speaker}_positive_sentiment_ratio'] = sentiment_counts.get('Positive', 0)\n",
    "        features[f'{speaker}_negative_sentiment_ratio'] = sentiment_counts.get('Negative', 0)\n",
    "    \n",
    "    df = df.sort_values('Start Time')\n",
    "    df['Next Start Time'] = df['Start Time'].shift(-1)\n",
    "    df['Response Time'] = df['Next Start Time'] - df['End Time']\n",
    "\n",
    "    #avg response time therapist to patient\n",
    "    therapist_to_patient = df[(df['Speaker'] == 'therapist') & (df['Speaker'].shift(-1) == 'patient')]\n",
    "    features['avg_response_time_therapist_to_patient'] = therapist_to_patient['Response Time'].mean()\n",
    "\n",
    "    #avg response time patient to therapist\n",
    "    patient_to_therapist = df[(df['Speaker'] == 'patient') & (df['Speaker'].shift(-1) == 'therapist')]\n",
    "    features['avg_response_time_patient_to_therapist'] = patient_to_therapist['Response Time'].mean()\n",
    "\n",
    "    #time alignment\n",
    "    df['Mid Time'] = (df['Start Time'] + df['End Time']) / 2\n",
    "    df = df.sort_values('Mid Time')\n",
    "    start_time = df['Mid Time'].min()\n",
    "    end_time = df['Mid Time'].max()\n",
    "    step = 1.0\n",
    "    common_times = np.arange(start_time, end_time + step, step)\n",
    "    \n",
    "    #resample to time grid\n",
    "    def resample_data(data):\n",
    "        resampled = pd.DataFrame(index=common_times, columns=data.columns)\n",
    "        for time in common_times:\n",
    "            mask = (data.index <= time)\n",
    "            if mask.any():\n",
    "                resampled.loc[time] = data[mask].iloc[-1]\n",
    "        return resampled.ffill()\n",
    "    \n",
    "    therapist_data = df[df['Speaker'] == 'therapist'].set_index('Mid Time')\n",
    "    patient_data = df[df['Speaker'] == 'patient'].set_index('Mid Time')\n",
    "    \n",
    "    therapist_resampled = resample_data(therapist_data)\n",
    "    patient_resampled = resample_data(patient_data)\n",
    "    \n",
    "    common_indices = therapist_resampled.index.intersection(patient_resampled.index)\n",
    "    therapist_resampled = therapist_resampled.loc[common_indices]\n",
    "    patient_resampled = patient_resampled.loc[common_indices]\n",
    "    \n",
    "    #remove nan\n",
    "    valid_indices = ~(np.isnan(therapist_resampled['Arousal']) | np.isinf(therapist_resampled['Arousal']) |\n",
    "                      np.isnan(patient_resampled['Arousal']) | np.isinf(patient_resampled['Arousal']) |\n",
    "                      np.isnan(therapist_resampled['Valence']) | np.isinf(therapist_resampled['Valence']) |\n",
    "                      np.isnan(patient_resampled['Valence']) | np.isinf(patient_resampled['Valence']))\n",
    "    \n",
    "    therapist_resampled = therapist_resampled[valid_indices]\n",
    "    patient_resampled = patient_resampled[valid_indices]\n",
    "    \n",
    "    #synchrony calculations with pearson correlation\n",
    "    if len(therapist_resampled) >= 2:\n",
    "        features['arousal_synchrony'], _ = stats.pearsonr(therapist_resampled['Arousal'], patient_resampled['Arousal'])\n",
    "        features['valence_synchrony'], _ = stats.pearsonr(therapist_resampled['Valence'], patient_resampled['Valence'])\n",
    "        therapist_sentiment = (therapist_resampled['Sentiment'] == 'Positive').astype(int)\n",
    "        patient_sentiment = (patient_resampled['Sentiment'] == 'Positive').astype(int)\n",
    "        features['sentiment_synchrony'], _ = stats.pearsonr(therapist_sentiment, patient_sentiment)\n",
    "    else:\n",
    "        features['arousal_synchrony'] = np.nan\n",
    "        features['valence_synchrony'] = np.nan\n",
    "        features['sentiment_synchrony'] = np.nan\n",
    "\n",
    "    plot_time_series(therapist_resampled, patient_resampled, 'Arousal', features['filename'])\n",
    "    plot_time_series(therapist_resampled, patient_resampled, 'Valence', features['filename'])\n",
    "    \n",
    "    #sentiment to numerical\n",
    "    therapist_sentiment = (therapist_resampled['Sentiment'] == 'Positive').astype(int)\n",
    "    patient_sentiment = (patient_resampled['Sentiment'] == 'Positive').astype(int)\n",
    "    \n",
    "    sentiment_df = pd.DataFrame({\n",
    "        'Therapist': therapist_sentiment,\n",
    "        'Patient': patient_sentiment\n",
    "    }, index=therapist_resampled.index)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.lineplot(data=sentiment_df)\n",
    "    plt.title(f'Sentiment Time Series for {features[\"filename\"]}')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Sentiment (0: Negative, 1: Positive)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'~/WAI/TimeSeriesFigs/{features[\"filename\"]}_Sentiment_timeseries.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"~/output/SpeakerSentAnalysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"~/WAI/TimeSeriesFigs/\", exist_ok=True)\n",
    "csv_files = glob.glob(os.path.join(data_folder, '*.csv'))\n",
    "all_features = [calculate_features(csv_file) for csv_file in csv_files]\n",
    "features_df = pd.DataFrame(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to align analysed transcripts with WAI scores\n",
    "def standardize_session_id(filename):\n",
    "    session_id = re.sub(r'_SentArVal\\.csv$', '', filename) #remove csv suffix\n",
    "    \n",
    "    session_id = re.sub(r'\\s+\\d{1,2}-\\d{1,2}-(\\d{2}|\\d{4})$', '', session_id) #remove dates from name\n",
    "    session_id = re.sub(r'\\s+\\d{4}-\\d{2}-\\d{2}$', '', session_id)\n",
    "\n",
    "    session_id = re.sub(r'\\bs(\\d+)', lambda m: f' sessie {int(m.group(1)):02d}', session_id) #replace s8 with sessie 08\n",
    "    \n",
    "    session_id = re.sub(r'sessie\\s*(\\d+)', lambda m: f'sessie {int(m.group(1)):02d}', session_id) #ensure 0 padded number\n",
    "    \n",
    "    return session_id.strip()\n",
    "    \n",
    "features_df['session_id'] = features_df['filename'].apply(standardize_session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_wai = pd.read_csv('~/WAI/patient_btg.csv')\n",
    "observer_wai = pd.read_csv('~/WAI/observer_btg.csv')\n",
    "therapist_wai = pd.read_csv('~/WAI/Therapist_ratings.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [patient_wai, therapist_wai, observer_wai]:\n",
    "    df['session_id'] = df['ppnr'].astype(str) + ' sessie ' + df['session'].astype(str).str.zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_wai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "therapist_wai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observer_wai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = features_df.merge(patient_wai[['session_id', 'bond', 'goal', 'task', 'wai']], on='session_id', suffixes=('', '_patient'))\n",
    "merged_df = merged_df.merge(therapist_wai[['session_id', 'bond', 'goal', 'task', 'wai']], on='session_id', suffixes=('', '_therapist'))\n",
    "merged_df = merged_df.merge(observer_wai[['session_id', 'bond', 'goal', 'task', 'wai']], on='session_id', suffixes=('', '_observer'))\n",
    "merged_df = merged_df.rename(columns={'bond':'bond_patient', 'goal':'goal_patient', 'task':'task_patient', 'wai':'wai_patient'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop_duplicates(subset='filename', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate patients manually, keep more recent entry (later session)\n",
    "merged_df = merged_df.drop([2,4,7,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('~/WAI/merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wai_columns = ['bond_patient', 'goal_patient', 'task_patient', 'wai_patient', \n",
    "               'bond_therapist', 'goal_therapist', 'task_therapist', 'wai_therapist',\n",
    "               'bond_observer', 'goal_observer', 'task_observer', 'wai_observer']\n",
    "\n",
    "feature_columns = [col for col in merged_df.columns if col not in wai_columns + ['filename', 'session_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive(merged_df):\n",
    "    print(\"Summary\")\n",
    "    print(merged_df.describe())\n",
    "\n",
    "    #readability font sizes\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 24,\n",
    "        'axes.titlesize': 22,\n",
    "        'axes.labelsize': 22,\n",
    "        'xtick.labelsize': 18,\n",
    "        'ytick.labelsize': 18,\n",
    "        'legend.fontsize': 22,\n",
    "        'figure.titlesize': 24\n",
    "    })\n",
    "\n",
    "    participant_types = ['patient', 'therapist', 'observer']\n",
    "    for participant in participant_types:\n",
    "        wai_cols = [col for col in wai_columns if col.endswith(participant)]\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "        fig.suptitle(f\"{participant.capitalize()} WAI Score Distributions\", fontsize=24)\n",
    "        for i, col in enumerate(wai_cols):\n",
    "            sns.histplot(merged_df[col], kde=True, ax=axes[i//2, i%2])\n",
    "            axes[i//2, i%2].set_title(col, fontsize=18)\n",
    "            axes[i//2, i%2].set_xlabel(axes[i//2, i%2].get_xlabel(), fontsize=20)\n",
    "            axes[i//2, i%2].set_ylabel(axes[i//2, i%2].get_ylabel(), fontsize=20)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig(f'~/WAI/{participant}_wai_score_distributions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    #divide into separate figures\n",
    "    feature_types = {\n",
    "        'therapist': [col for col in feature_columns if col.startswith('therapist')],\n",
    "        'patient': [col for col in feature_columns if col.startswith('patient')],\n",
    "        'miscellaneous': [col for col in feature_columns if not (col.startswith('therapist') or col.startswith('patient'))]\n",
    "    }\n",
    "\n",
    "    for feature_type, features in feature_types.items():\n",
    "        num_features = len(features)\n",
    "        num_cols = 4\n",
    "        num_rows = (num_features + num_cols - 1) // num_cols\n",
    "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 6*num_rows))\n",
    "        fig.suptitle(f\"{feature_type.capitalize()} Feature Distributions\", fontsize=24)\n",
    "        for i, feature in enumerate(features):\n",
    "            sns.histplot(merged_df[feature], kde=True, ax=axes[i//num_cols, i%num_cols])\n",
    "            axes[i//num_cols, i%num_cols].set_title(feature, fontsize=16)\n",
    "            axes[i//num_cols, i%num_cols].set_xlabel(axes[i//num_cols, i%num_cols].get_xlabel(), fontsize=16)\n",
    "            axes[i//num_cols, i%num_cols].set_ylabel(axes[i//num_cols, i%num_cols].get_ylabel(), fontsize=16)\n",
    "        for i in range(num_features, num_rows * num_cols):\n",
    "            fig.delaxes(axes[i//num_cols, i%num_cols])\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig(f'~/WAI/{feature_type}_feature_distributions.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptive(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_calc(df, wai_columns, feature_columns):\n",
    "    correlations = {}\n",
    "    for wai_col in wai_columns:\n",
    "        for feature_col in feature_columns:\n",
    "            valid_data = df[[wai_col, feature_col]].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            if len(valid_data) > 1:\n",
    "                corr, p_value = stats.pearsonr(valid_data[wai_col], valid_data[feature_col])\n",
    "                correlations[f\"{wai_col}_{feature_col}\"] = {'correlation': corr, 'p_value': p_value}\n",
    "            else:\n",
    "                correlations[f\"{wai_col}_{feature_col}\"] = {'correlation': np.nan, 'p_value': np.nan}\n",
    "    return pd.DataFrame(correlations).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = correlation_calc(merged_df, wai_columns, feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations['abs_correlation'] = abs(correlations['correlation'])\n",
    "correlations_sorted = correlations.sort_values('abs_correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlations_sorted.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_sorted.to_csv('~/WAI/wai_feature_correlations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df[feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_models(X, y, target_name, max_k=5):\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    models = {\n",
    "        'Mean': lambda: np.mean(y),\n",
    "        'Linear': LinearRegression(),\n",
    "        'SVR': SVR(kernel='rbf')\n",
    "    }\n",
    "    \n",
    "    for k in range(1, max_k + 1):\n",
    "        models[f'KNN (k={k})'] = KNeighborsRegressor(n_neighbors=k)\n",
    "    \n",
    "    results = {model_name: [] for model_name in models}\n",
    "    \n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            if model_name == 'Mean':\n",
    "                y_pred = model()\n",
    "            else:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "            \n",
    "            mse = mean_squared_error([y_test.iloc[0]], [y_pred[0]] if hasattr(y_pred, '__iter__') else [y_pred])\n",
    "            results[model_name].append(np.sqrt(mse))\n",
    "    \n",
    "    avg_results = {model_name: np.mean(scores) for model_name, scores in results.items()}\n",
    "    \n",
    "    #best knn\n",
    "    knn_results = {k: avg_results[f'KNN (k={k})'] for k in range(1, max_k + 1)}\n",
    "    best_k = min(knn_results, key=knn_results.get)\n",
    "    best_knn_rmse = knn_results[best_k]\n",
    "    \n",
    "    final_results = {\n",
    "        'target': target_name,\n",
    "        'Mean': avg_results['Mean'],\n",
    "        'Linear': avg_results['Linear'],\n",
    "        'SVR': avg_results['SVR'],\n",
    "        'KNN': {\n",
    "            'best_k': best_k,\n",
    "            'best_rmse': best_knn_rmse\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return final_results, knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []\n",
    "knnr_results = {}\n",
    "for target in wai_columns:\n",
    "    y = merged_df[target]\n",
    "    result, knn = final_models(X, y, target)\n",
    "    model_results.append(result)\n",
    "    knnr_results[target] = knn\n",
    "\n",
    "model_results_df = pd.DataFrame(model_results)\n",
    "print(model_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_df.to_csv('~/WAI/model_rmse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "fig.suptitle('KNN Regression Performance for WAI Targets', fontsize=22)\n",
    "\n",
    "for i, (target, results) in enumerate(knnr_results.items()):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    k_values = list(results.keys())\n",
    "    rmse_values = [results[k] for k in k_values]\n",
    "    \n",
    "    ax.plot(k_values, rmse_values, marker='s', label='RMSE')\n",
    "    ax.set_title(target)\n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"~/WAI/KNNWAI.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Len_venv3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
